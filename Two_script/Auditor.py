import os
import sys
import json
import textwrap
from pathlib import Path
import pdfplumber
import google.genai as genai
from google.genai import types

from KEY import GEMINI_PRO_API_KEY

RESET = "\033[0m"  # Reset to default
RED = "\033[31m"   # Foreground red
GREEN = "\033[32m" # Foreground green

API_KEY = GEMINI_PRO_API_KEY

if not API_KEY:
    print(f"{RED} CRITICAL: API key not found. Please set it in the script{RESET}")
    sys.exit(-1)

client = genai.Client(api_key=API_KEY)
model_name = "gemini-2.5-pro"
gen_config=types.GenerateContentConfig(
            max_output_tokens=65536,
            top_k=2,
            top_p=0.5,
            temperature=0.5,
            response_mime_type='application/json'
        )


def load_user_documents( dir_path: str) -> dict[str, str]:
        """
        Loads multiple user PDFs from a directory using pdfplumber to extract both text and tables, converting tables to Markdown.

        Returns:
            A dictionary where keys are filenames and values are their full text and Markdown-formatted table content.
        """
        if not os.path.isdir(dir_path):
            print(f"{RED} Error: Directory '{dir_path}' is not a valid Folder path.\n{RESET}")
            sys.exit(" Exiting due  to invalid directory.")

        user_docs = {}
        print(f"\n Scanning for user documents in: {dir_path}")
        for filepath in Path(dir_path).glob('*.pdf'):
           # if not filepath.is_file():
            #    continue

            print(f" {GREEN} - Processing: {filepath.name}")
            try:
                full_content = ""
                with pdfplumber.open(filepath) as pdf:
                    for i, page in enumerate(pdf.pages):
                        # Extract plain text from the page
                        page_text = page.extract_text()
                        if page_text:
                            full_content += f"\n\n--- Page {i + 1} Text ---\n"
                            full_content += page_text

                        # Extract tables and convert them to Markdown format
                        page_tables = page.extract_tables()
                        if page_tables:
                            full_content += f"\n\n--- Page {i + 1} Tables ---\n"
                            for table in page_tables:
                                # Clean up table data, replacing None with empty strings
                                clean_table = [list(map(lambda x: str(x) if x is not None else '', row)) for row in
                                               table]
                                if not clean_table: continue

                                # Convert to Markdown
                                header = "| " + " | ".join(clean_table[0]) + " |"
                                separator = "| " + " | ".join(['---'] * len(clean_table[0])) + " |"
                                body = "\n".join(["| " + " | ".join(row) + " |" for row in clean_table[1:]])

                                full_content += f"\n{header}\n{separator}\n{body}\n"

                if full_content.strip():
                    user_docs[filepath.name] = full_content
                    print(f"    Successfully loaded and parsed.{RESET}")
                else:
                    print(f"    Skipped empty or unreadable document.")

            except Exception as e:
                print(f"    {RED} Error loading {filepath.name}: {e} {RESET}")

        if not user_docs:
            sys.exit(f"{RED}No user documents were successfully loaded. Exiting{RESET}")
        return user_docs


def load_knowledge_base(filepath: str = "aspice_knowledge_base.json") -> dict:
#fectehes the json generated by indexer script

    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"{RED} CRITICAL ERROR: The knowledge base file '{filepath}' was not found.\n\nRun the indexer script first to generate this file. {RESET}")
        sys.exit(2)
    except json.JSONDecodeError:
        print(f"{RED} CRITICAL ERROR: The knowledge base file '{filepath}' is corrupted or not a valid JSON.{RESET}")
        sys.exit(2)


def _construct_auditor_prompt(R_BP: list, R_GP: list, user_docs: dict[str, str], domain: str, level: str) -> str:

    bp_string = json.dumps(R_BP, indent=0)
    gp_string = json.dumps(R_GP, indent=0)

    clean_bp_string = bp_string.replace("{", "") \
        .replace("}", "") \
        .replace("[", "") \
        .replace("]", "")
    clean_gp_string = gp_string.replace("{", "") \
        .replace("}", "") \
        .replace("[", "") \
        .replace("]", "")

    user_docs_formatted = "\n\n---\n\n".join([f"DOCUMENT NAME: {name}\n\nCONTENT:\n{content}" for name, content in user_docs.items()])

    prompt = f"""
    ROLE: You are an expert Compliance Auditor specializing in the Automotive SPICE standards and Gap Analysis. 
    
    GOAL: Your task is to perform a detailed compliance check of the user's documents against a specific set of practices for a given process and capability level. You will also check for ALL the gaps in the user documents and provide apt solutions to fill them.
    
    CONTEXT:
    - Process to Assess: {domain}
    - Target Capability Level: {level}

    REQUIREMENTS FOR THIS AUDIT:
    You are to audit ONLY against the following specific practices. Do not infer other requirements. You must mention the BPs and GPs which you cannot perform analysis for due to lack of documents, at the top of your report 

    Base Practices for {domain}: {clean_bp_string}
    Generic Practices for Level {level}: {clean_gp_string}

    YOUR TASK:
    1.  Analyze and Score: For EACH Base Practice and Generic Practice listed above:
        a. Meticulously search ALL provided user documents for evidence that the practice is being performed.
        b. Provide a Score: Use the rating method given in BP_rating_rules or GP_rating _rules for that specific BP or GP: N (Not achieved), P (Partially achieved), L (Largely achieved), F (Fully achieved).
        c. Provide Detailed Reasoning: Explain the reasoning behind what documents you checked for, what you looked for withing the document and if the document is not compliant with the standards, explain why.
           - If evidence is found, QUOTE the relevant text directly from the user document(s), state the document name, and explain HOW this quote satisfies the practice.
           - If evidence is missing or partial, clearly state WHAT is missing.
    2.  Gap Analysis: For any practice scoring below 'F', describe ALL the gaps in the document with respective artifact ID provided in the user documents.
    3.  Provide Suggestions: For each identified gap, give clear, actionable suggestions on what to create or change.

    OUTPUT FORMAT:
    The JSON object must contain a single top-level key named "Report", whose value is a string containing your analysis in structured Markdown. Use the exact following format:

    Report
        Brief bullet points for BPs/GPs which cannot be assessed due to lack of documents, also mention the documents required.
        
        "ASSESSMENT REPORT START"
        Process Assessed: {domain}
        Target Capability Level: {level}

        Base Practices (BPs) for {domain}
            BP.X: [Base Practice Title]
            - Score: [N, P, L, F]
            - Reasoning: [Your detailed explanation, including quotes and document references.]
            - Gap Analysis: [Description of the gap.]
            - Suggestions: [Actionable advice to close the gap.]

        Generic Practices (GPs) for Level {level}
            GP X.Y.Z: [Generic Practice Title]
            - Score: [N, P, L, F]
            - Reasoning: [Your detailed explanation, including quotes and document references.]
            - Gap Analysis: [Description of the gap.]
            - Suggestions: [Actionable advice to close the gap.]
        "ASSESSMENT REPORT END"
        
        Tabular data with BP/GP ID along with their ratings. Leave a "-" for ratings not at all assessed.
               
    PROVIDED USER DOCUMENTS FOR YOUR ANALYSIS:
    {user_docs_formatted}
    """
    return textwrap.dedent(prompt)


def analyze_compliance(prompt : str) -> str:

    print("\n Requesting AI analysis... This may take a few moments.")

    response = client.models.generate_content(
            model=model_name,
            contents=prompt,
            config=gen_config
    )
    print(f"{GREEN} AI analysis complete.{RESET}")
    raw_json_string = response.candidates[0].content.parts[0].text

    try:
        data = json.loads(raw_json_string)
        report_content = data.get("Report", "ERROR: 'Report' key not found in the AI's JSON response.")
        return report_content

    except json.JSONDecodeError:
        print(f"{RED}Error: Failed to decode JSON from AI. Printing raw response{RESET}\n\n{raw_json_string}")
        sys.exit(1)

    except Exception as e:
        print(f"{RED}\nAn unexpected error occurred during AI analysis:{e}{RESET}")
        sys.exit(1)


def main():

    print("=" * 100)
    print("       Agentic AI - Automotive SPICE (ASPICE) Compliance Auditor ")
    print("=" * 100)

    knowledge_base_dict = load_knowledge_base()

    try:
#Load user documents
        user_docs_dir = input(" Enter the path to the folder containing your project documents: ").strip()
        user_docs_content = load_user_documents(user_docs_dir)

#Ensure Domain and Level exist in knowledgebase
        while True:
            domain = input(f"{RESET} Which Process Area do you want to check? (e.g., SYS.2, SYS.3): ").strip().upper()
            level = input(" What Capability Level do you want to check against? (e.g., 1, 2): ").strip()
            if domain not in knowledge_base_dict or level not in knowledge_base_dict.get("CapabilityLevel", {}):
                f = input(f" Domain {domain} or Level {level} not found in knowledge base. Ensure domain and level are valid.\n{RED} Do you want to quit program? y/n {RESET}").strip().lower()
                if f == "y":
                    sys.exit("Exiting program")
            else: break

#get relevant BPs and GPs
        relevant_bps = knowledge_base_dict[domain]
        relevant_gps = []
        for i in range(1, int(level) + 1):
            level_str = str(i)
            gps_for_level = knowledge_base_dict["CapabilityLevel"][level_str].get("GenericPractices", [])
            relevant_gps.extend(gps_for_level)

#call functions to make prompt and call analyser
        prompt = _construct_auditor_prompt(
                R_BP = relevant_bps,
                R_GP = relevant_gps,
                user_docs=user_docs_content,
                domain=domain,
                level=level
        )

        final_report = analyze_compliance(prompt)

        print("\n\n" + "=" * 43 + " FINAL REPORT " + "=" * 43 + "\n")
        print(final_report)
        print("\n" + "=" * 100)

        report_filename = f"ASPICE_Report_{domain.replace('.', '_')}_Level{level}.md"
        with open(report_filename, "w", encoding='utf-8') as f:
            f.write(final_report)
        print(f"\n {GREEN} Report saved to: {report_filename}{RESET}")

    except KeyboardInterrupt:
        print(f"\n {RED} Program terminated by user.{RESET}")
    except Exception as e:
        sys.exit(f"\n{RED} Error occurred in the main flow: {e}{RESET}")


if __name__ == "__main__":
    main()
